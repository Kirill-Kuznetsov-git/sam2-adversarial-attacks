{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b28288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from PIL import Image\n",
    "from segmentation_models_pytorch.losses import DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a15e2f-c7e1-4e5d-862f-fcb751a60b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:2\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:2\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# отключили автокаст, тк бекворд не работает с ним\n",
    "# if device.type == \"cuda\":\n",
    "#     # use bfloat16 for the entire notebook\n",
    "#     torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "#     # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "#     if torch.cuda.get_device_properties(0).major >= 8:\n",
    "#         torch.backends.cuda.matmul.allow_tf32 = True\n",
    "#         torch.backends.cudnn.allow_tf32 = True\n",
    "# elif device.type == \"mps\":\n",
    "#     print(\n",
    "#         \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "#         \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "#         \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29bc90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(3)\n",
    "\n",
    "# def show_mask(mask, ax, random_color=False, borders = False):\n",
    "#     if random_color:\n",
    "#         color = np.concatenate([np.rZandom.random(3), np.array([0.6])], axis=0)\n",
    "#     else:\n",
    "#         color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "#     h, w = mask.shape[-2:]\n",
    "#     mask = mask.astype(np.uint8)\n",
    "#     mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "#     if borders:\n",
    "#         import cv2\n",
    "#         contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "#         # Try to smooth contours\n",
    "#         contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "#         mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) \n",
    "#     ax.imshow(mask_image)\n",
    "\n",
    "# def show_points(coords, labels, ax, marker_size=375):\n",
    "#     pos_points = coords[labels==1]\n",
    "#     neg_points = coords[labels==0]\n",
    "#     ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "#     ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "\n",
    "# def show_box(box, ax):\n",
    "#     x0, y0 = box[0], box[1]\n",
    "#     w, h = box[2] - box[0], box[3] - box[1]\n",
    "#     ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2)) \n",
    "\n",
    "# def show_box(box, image):\n",
    "#     x0, y0 = box[0], box[1]\n",
    "#     w, h = box[2] - box[0], box[3] - box[1]\n",
    "#     image_box = cv2.rectangle(image, (int(x0), int(y0)), (int(x0 + w), int(y0 + h)), (255, 0, 0), 2)\n",
    "#     return image_box, image\n",
    "\n",
    "# def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=False):\n",
    "#     for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "#         plt.figure(figsize=(10, 10))\n",
    "#         plt.imshow(image)\n",
    "#         show_mask(mask, plt.gca(), borders=borders)\n",
    "#         if point_coords is not None:\n",
    "#             assert input_labels is not None\n",
    "#             show_points(point_coords, input_labels, plt.gca())\n",
    "#         if box_coords is not None:\n",
    "#             # boxes\n",
    "#             show_box(box_coords, plt.gca())\n",
    "#         if len(scores) > 1:\n",
    "#             plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "\n",
    "# def show_mask(mask_n, image):\n",
    "#     print(mask_n.shape)\n",
    "#     print(image.shape)\n",
    "#     image_masked = cv2.addWeighted(image, 0.5, mask_n, 0.5, 0)\n",
    "#     return image_masked\n",
    "\n",
    "# def show_mask(mask, image):\n",
    "#     image_n = image.copy(deep = True)\n",
    "#     mask_n = mask.copy(deep = True)\n",
    "#     image_n[mask_n == 255] = (0, 255, 0)\n",
    "#     return image_n, image\n",
    "\n",
    "def save_mask(image, mask, box, iter, alpha=0.7):\n",
    "    image = (np.array(image) / 255.).transpose(2, 0, 1)\n",
    "    mask = np.stack([mask[0] * 87./255, mask[0] * 186./255, mask[0] * 168./255])\n",
    "\n",
    "    image_transparency = (image * (1 - alpha) + mask * alpha).clip(0, 1)\n",
    "\n",
    "    image = np.where(mask, image_transparency, image).clip(0, 1)\n",
    "    image = Image.fromarray((image.transpose(1, 2, 0) * 255).astype(dtype=np.uint8))\n",
    "    \n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    image_box = cv2.rectangle(image, (int(x0), int(y0)), (int(x0 + w), int(y0 + h)), (255, 0, 0), 2)\n",
    "    \n",
    "    image.save('./bbox_frames/' + str(iter).zfill(4) + '.jpg')\n",
    "    \n",
    "    \n",
    "# def save_masks(image, masks, box_coords=None, borders=False):\n",
    "#     fig = plt.figure(figsize=(image.shape[1]/100, image.shape[0]/100))\n",
    "#     plt.imshow(image)\n",
    "#     show_mask(mask, plt.gca(), borders=borders)\n",
    "#     if box_coords is not None:\n",
    "#         show_box(box_coords, plt.gca())\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     # fig = plt.gcf()\n",
    "#     fig.canvas.draw()\n",
    "#     canvas = FigureCanvas(fig)\n",
    "#     canvas.draw()\n",
    "#     plt.close()\n",
    "#     img = np.array(fig.canvas.renderer.buffer_rgba())[:,:,:3]\n",
    "#     img = cv2.cvtColor(np.uint8(img*255), cv2.COLOR_RGB2BGR)\n",
    "#     # print(img.shape)\n",
    "#     plt.clf()\n",
    "#     return img\n",
    "\n",
    "# def save_masks(image, mask, box_coords):\n",
    "#     image_mask, image = show_mask(mask, image)\n",
    "#     cv2.imwrite('./bbox_frames/' + str(iter).zfill(4) + 'image1' +'.jpg', image)\n",
    "#     img_box, image = show_box(box_coords, image_mask)\n",
    "#     cv2.imwrite('./bbox_frames/' + str(iter).zfill(4) + 'image2' +'.jpg', image)\n",
    "#     return img_box, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94cc19e7-bdaf-4688-a512-eb093760c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square (coords):\n",
    "    return (coords[2] - coords[0]) * (coords[3] - coords[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c2e4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('../images/GrabCut/data_GT/sheep.jpg')\n",
    "image = np.array(image.convert(\"RGB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d2e74a2-0c7c-4ffa-be2d-81760b8f2b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 450\n"
     ]
    }
   ],
   "source": [
    "h, w = image.shape[:2]\n",
    "print(h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2b2dc0d-0cec-4f0b-9929-bf8086896e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mask = Image.open('../images/GrabCut/boundary_GT/sheep.bmp')\n",
    "zero_mask = np.array(zero_mask)\n",
    "zero_mask = torch.as_tensor(zero_mask, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e28150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "sam2_checkpoint = \"../../checkpoints/sam2_hiera_large.pt\"\n",
    "model_cfg = \"sam2_hiera_l.yaml\"\n",
    "\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
    "\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "\n",
    "# убираем отслеживание всех параметров\n",
    "for param in predictor.model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d95d48dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6923b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_box = np.array([177, 182, 296, 378])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9fec49e-6dc2-4fe5-9b55-8da63b7784f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# зачем\n",
    "mask_input, unnorm_coords, labels, unnorm_box_input = predictor._prep_prompts(\n",
    "    point_coords=None, point_labels=None, box=input_box, mask_logits=None, normalize_coords=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b0a0b08-d29b-4064-ad74-63ef4c9dbcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[402.7733, 310.6133],\n",
       "         [673.5645, 645.1200]]], device='cuda:2')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unnorm_box_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e0ebda0-cb1d-44da-a8d1-622279f05ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxes_n = torch.as_tensor(input_box, dtype=torch.float, device=predictor.device)\n",
    "# boxes = boxes_n.reshape(-1, 2, 2)\n",
    "# coords = boxes.clone()\n",
    "# coords[..., 0] = coords[..., 0] / w\n",
    "# print(coords)\n",
    "# coords[..., 1] = coords[..., 1] / h\n",
    "# print(coords)\n",
    "# coords = coords * 1024  # хз почему, мб макс разрешение sam2\n",
    "# print(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "120d3fa0-0ecd-4aa8-a614-12341f8686fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# делаем обратную нормализацию координат\n",
    "def back_norm(unnorm_box, w, h):\n",
    "    norm_box = unnorm_box/1024\n",
    "    coords = norm_box.clone()\n",
    "    coords[..., 0] = coords[..., 0] * w\n",
    "    coords[..., 1] = coords[..., 1] * h\n",
    "    box_coord = coords.squeeze(0).float().detach().cpu().numpy().flatten()\n",
    "    return box_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19d42360-603e-4c4f-958b-9bb6ad7b57c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.MSELoss()\n",
    "criterion = DiceLoss('binary') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b05940f-a5c2-4d92-9f8f-3677246c136b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3717263736.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[31], line 29\u001b[0;36m\u001b[0m\n\u001b[0;31m    save_mask(image: Image, mask: np.ndarray, unnorm_coords_np, iter, alpha=0.7):\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "iters = 200\n",
    "lambda_reg = 0.01\n",
    "\n",
    "unnorm_box = unnorm_box_input\n",
    "unnorm_box.requires_grad = True\n",
    "opt = optim.Adam([unnorm_box], lr=1e-2)\n",
    "    \n",
    "for iter in range(iters):\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    masks, iou_predictions, low_res_masks = predictor._predict(\n",
    "        unnorm_coords,\n",
    "        labels,\n",
    "        unnorm_box,\n",
    "        mask_input,\n",
    "        multimask_output=False,\n",
    "        return_logits=True\n",
    "    )\n",
    "\n",
    "    masks_np = (masks>0.0).squeeze(0).detach().cpu().numpy()*255\n",
    "    iou_predictions_np = iou_predictions.squeeze(0).float().detach().cpu().numpy()\n",
    "    low_res_masks_np = low_res_masks.squeeze(0).float().detach().cpu().numpy()\n",
    "    unnorm_coords_np = back_norm(unnorm_box, w, h)\n",
    "\n",
    "    masks_np2 = masks_np.squeeze(0)\n",
    "    masks_loss = masks.cpu()\n",
    "    zero_mask_torch = zero_mask.unsqueeze(0).cpu()\n",
    "\n",
    "    save_mask(image: Image, mask: np.ndarray, unnorm_coords_np, iter, alpha=0.7):\n",
    "    \n",
    "    # loss = criterion(masks.squeeze().cpu(), zero_mask)\n",
    "    # print(masks)\n",
    "    # print(zero_mask_torch)\n",
    "    loss = criterion(masks_loss, zero_mask_torch)\n",
    "    reg_loss = abs(square(input_box)-square(unnorm_coords_np))\n",
    "    total_loss = loss + lambda_reg * reg_loss**2\n",
    "    \n",
    "    if iter % 100 == 0:\n",
    "        print(loss)\n",
    "    \n",
    "    total_loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f8eaf-cec9-4610-9859-b9ded5efa296",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -i /home/user20/segment-anything-2/notebooks/sam2_adversarial_attacks/bbox_frames/%04d.jpg -r 30 bbox_video/out17.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c9994-d3e8-4538-b189-3c207ab12ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650ddd02-97f8-46de-9cd4-4de33a6708a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "liza_env",
   "language": "python",
   "name": "liza_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
